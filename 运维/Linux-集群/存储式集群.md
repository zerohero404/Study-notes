<img width="866" height="554" alt="Linux：集群_51" src="https://github.com/user-attachments/assets/5054912f-d2dd-400b-98d3-03e6060f2e7a" /># 1.集群化存储概述
## 1.1 用于存储的网络技术
- DAS Direct-Attached Storage<br>
&ensp;&ensp;说明：直接附加存储，等同于存储设备通过数据线、光缆、SATA 直连主机<br>
&ensp;&ensp;优点:技术简单、传输效率最高<br>
&ensp;&ensp;缺点:存储设备与主机相互绑定，不利于后期扩展与共享<br>
-NAS (Network Attached Storage)<br>
&ensp;&ensp;说明：网络附加存储，通过网络在存储主机与使用主机之间传输数据<br>
&ensp;&ensp;优点:技术相对简单、不要求存储设备直连本机，只需在局域网下即可<br>
&ensp;&ensp;缺点:存储速率较慢<br>
&ensp;&ensp;网络拓扑<br>
 <img width="428" height="390" alt="Linux：集群_31" src="https://github.com/user-attachments/assets/2f39aff1-d4f8-48e0-8f5f-51b0ea362a78" /><br>
- SAN (Storage Area Network)<br>
&ensp;&ensp;说明：存储区域网络，将生产网络与存储网络进行隔离，有效增加各部效率，减轻网络设备压力，适合大并发业务<br>
&ensp;&ensp;优点:存储安全性较高、存储速率较高<br>
&ensp;&ensp;缺点:造价昂贵、技术难度相对较高<br>
&ensp;&ensp;网络拓扑<br>
 <img width="638" height="478" alt="Linux：集群_32" src="https://github.com/user-attachments/assets/affab867-672d-4a19-8903-a3bb508389f4" /><br>
## 1.2 实现存储的技术分类
- 块存储<br>
&ensp;&ensp;常见设备:<br>
 <img width="326" height="341" alt="Linux：集群_33" src="https://github.com/user-attachments/assets/51b91231-7e45-4875-a190-588113c02226" /><br>
&ensp;&ensp;优点:可用通过 Raid、LVM 等简单技术实现高可用、可独立使用<br>
&ensp;&ensp;缺点:不利于在多台网络设备之间进行共享<br>
- 文件存储、网络存储<br>
&ensp;&ensp;常见方法:SAMBA、NFS<br>
&ensp;&ensp;优点:构建资金低、可在不同主机之间共享存储<br>
&ensp;&ensp;缺点:读写效率低，传输效率慢<br>
- 对象存储、分布式存储、存储桶<br>
&ensp;&ensp;常见设备:
 <img width="494" height="589" alt="Linux：集群_34" src="https://github.com/user-attachments/assets/4e3baf51-2357-43d8-8cd2-da8317a4dbdb" /><br>
&ensp;&ensp;优点:读写效率高、可在不同主机之间共享存储<br>
&ensp;&ensp;缺点:造价昂贵，技术实现难度较高<br>
- 常见的一些存储名词以及背后使用技术对应关系<br>
&ensp;&ensp;文件存储、网络存储  FTPSAMBA、Apache、Nginx<br>
&ensp;&ensp;对象存储、分布式存储、云存储  CEPH、MooseFS、RHCS、ClusterFS<br>
&ensp;&ensp;块存储  ISCSI<br>
## 1.3 存储实现技术与存储网络之间的联系
- 块存储 + DAS<br>
&ensp;&ensp;结构描述：块存储直连主机设备是最常见的存储方式，也是后边所有存储方式的基石<br>
&ensp;&ensp;结构拓扑:<br>
 <img width="672" height="464" alt="Linux：集群_35" src="https://github.com/user-attachments/assets/88e6c22a-9a5e-46b0-912d-158e8ca76e1e" /><br>
- 块存储 + NAS<br>
&ensp;&ensp;结构描述：将块设备通过局域网络共享至其它使用主机节点，提升空间利用率，便于后期扩展<br>
&ensp;&ensp;结构拓扑:<br>
 <img width="531" height="271" alt="Linux：集群_36" src="https://github.com/user-attachments/assets/1acc23e0-8959-49aa-9be6-89cc1818c22d" /><br>
- 块存储 + SAN<br>
&ensp;&ensp;结构描述：将块设备通过局域网络共享至其它使用主机节点，提升空间利用率。并且将用户访问网络与存储网络相隔离，利用提升存储效率以后后期维护扩展<br>
&ensp;&ensp;结构拓扑<br>
 <img width="713" height="453" alt="Linux：集群_37" src="https://github.com/user-attachments/assets/a564eb18-1f75-49f3-9e1b-b695fb2dc711" /><br>
- 文件存储 + NAS<br>
&ensp;&ensp;结构描述：利用共享服务将文件通过网络传输至使用设备，便于文件在多台机器之间的共享操作<br>
&ensp;&ensp;结构拓扑<br>
 <img width="680" height="398" alt="Linux：集群_38" src="https://github.com/user-attachments/assets/ef4a7eb3-3345-4e85-8fcb-1f701a991c1b" /><br>
- 对象存储 + SAN<br>
&ensp;&ensp;结构描述：利用分布式文件存储结构提升文件读写并发效率，并且将用户访问网络与存储网络相隔离， 利用提升存储效率以后后期维护扩展<br>
&ensp;&ensp;结构拓扑<br>
 <img width="767" height="399" alt="Linux：集群_39" src="https://github.com/user-attachments/assets/490e88c4-e596-4f68-b424-d81ed66517fa" /><br>

# 2. 文件存储 – NFS
## 2.1 NFS 概述
- 什么是 NFS ？<br>
&ensp;&ensp;NFS（Network File System）即网络文件系统，是 FreeBSD 支持的文件系统中的一种，它允许网络中的计算机之间通过TCP/IP 网络共享资源。在 NFS 的应用中，本地 NFS 的客户端应用可以透明地读写位于远端NFS 服务器上的文件，就像访问本地文件一样。NFS 最早是由Sun 公司发展出来的，后被逐渐完整以及整合至 Linux 内核<br>
- NFS 功能<br>
&ensp;&ensp;它就是是可以透过网络，让不同的主机、不同的操作系统可以共享存储<br>
- NFS 原理<br>
&ensp;&ensp;NFS 在文件传送或信息传送过程中依赖于 RPC 协议。远程过程调用 (Remote Procedure Call) 是能使客户端执行其他系统中程序的一种机制所以只要用到 NFS 的地方都要启动 RPC 服务，不论是 NFS SERVER 或者 NFS CLIENT 。这样 SERVER 和 CLIENT 才能通过 RPC 来实现 PROGRAM PORT的 对应。可以这么理解 RPC 和 NFS 的关系：NFS 是一个文件系统，而 RPC  是负责负责信息的传输<br>
- NFS 优点<br>
&ensp;&ensp;节省本地存储空间，将常用的数据存放在一台 NFS  服务器上且可以通过网络访问，那么本地终端将可以减少自身存储空间的使用<br>
&ensp;&ensp;用户不需要在网络中的每个机器上都建有 Home 目录，Home 目录可以放在 NFS 服务器上且可以在网络上被访问使用<br>
&ensp;&ensp;一些存储设备 CDROM 和 Zip 等都可以在网络上被别的机器使用。这可以减少整个网络上可移动介质设备的数量<br>
## 2.2 NFS 配置
- NFC 服务器
&ensp;&ensp;yum –y install rpcbind nfs-utils # 服务器端安装 rpcbind 和 NFS-utils 软件包
&ensp;&ensp;/etc/init.d/rpcbind status
&ensp;&ensp;先创建 /file/nfs-1 /file/nfs-2 两个文件夹
&ensp;&ensp;chown -R nobody:nobody /file/nfs-1/ /file/nfs-2/
&ensp;&ensp;vim /etc/exports # 里面文件是空白的

```bash
NFS 共享目录 NFS 客户端地址 1(参数 1,参数 2,参数 3……)  客户端地址 2(参数 1,参数 2,参数3……. )
NFS 共享目录 NFS 客户端地址(参数 1,参数 2,参数 3…… )
```
 <img width="380" height="42" alt="Linux：集群_40" src="https://github.com/user-attachments/assets/776e5caa-6e0e-47fd-9ef6-6feaec4f6d18" /><br>
&ensp;&ensp;service rpcbind restart<br>
&ensp;&ensp;service nfs restart<br>
&ensp;&ensp;showmount -e 10.10.10.11（NFC 服务器 IP） # 查看挂载情况<br>

- 客户端
&ensp;&ensp;mkdir /file-salve/nfs-1<br>
&ensp;&ensp;mkdir /file-salve/nfs-2<br>
&ensp;&ensp;先创建两个挂载文件夹<br>
&ensp;&ensp;chown -R nobody:nobody /file-salve/nfs-1/ /file-salve/nfs-2<br>
&ensp;&ensp;mount -t nfs 10.10.10.11（NFC 服务器 IP）:/file/nfs-1（文件路径） /file-salve/nfs-1/（本机要挂载的路径）<br>
&ensp;&ensp;mount -t nfs 10.10.10.11（NFC 服务器 IP）:/file/nfs-2（文件路径） /file-salve/nfs-2/（本机要挂载的路径）<br>
- /etc/exports 参数规则<br>
&ensp;&ensp;rw：read-write，可读写，注意，仅仅这里设置成读写客户端还是不能正常写入，还要正确地设置共享目录的权限<br>
&ensp;&ensp;ro：read-only，只读<br>
&ensp;&ensp;sync：文件同时写入硬盘和内存<br>
&ensp;&ensp;async：文件暂存于内存，而不是直接写入内存<br>
&ensp;&ensp;no_root_squash：NFS 客户端连接服务端时如果使用的是 root 的话，那么对服务端分享的目录来说， 也拥有 root 权限。显然开启这项是不安全的<br>
&ensp;&ensp;root_squash：NFS 客户端连接服务端时如果使用的是 root 的话，那么对服务端分享的目录来说，拥有匿名用户权限，通常他将使用 nobody 或 nfsnobody 身份<br>
&ensp;&ensp;all_squash：不论 NFS 客户端连接服务端时使用什么用户，对服务端分享的目录来说都是拥有匿名用户权限<br>
&ensp;&ensp;anonuid：匿名用户的 UID  值，通常是 nobody 或 nfsnobody，可以在此处自行设定<br>
&ensp;&ensp;anongid：匿名用户的GID 值<br>

# 3. 分布式存储 – MFS
## 3.1 什么是 MFS？
- MFS 相关介绍<br>
&ensp;&ensp;MooseFS 是一个具备冗余容错功能的分布式网络文件系统，它将数据分别存放在多个物理服务器或单独磁盘或分区上，确保一份数据有多个备份副本，然而对于访问 MFS 的客户端或者用户来说，整个分布式网络文件系统集群看起来就像一个资源一样，从其对文件系统的情况看 MooseFS 就相当于UNIX 的文件系统<br>
- MFS 的特性说明<br>
&ensp;&ensp;高可靠性：每一份数据可以设置多个备份（多分数据），并可以存储在不同的主机上<br>
&ensp;&ensp;高可扩展性：可以很轻松的通过增加主机的磁盘容量或增加主机数量来动态扩展整个文件系统的存储量<br>
&ensp;&ensp;高可容错性：我们可以通过对 mfs 进行系统设置，实现当数据文件被删除后的一段时间内，依旧存放于主机的回收站中，以备误删除恢复数据<br>
&ensp;&ensp;高数据一致性：即使文件被写入、访问时，我们依然可以轻松完成对文件的一致性快照<br>
- MFS 缺点<br>
&ensp;&ensp;master 目前是单点，虽然会把数据信息同步到备份服务器，但是恢复需要时间<br>
&ensp;&ensp;master 服务器对主机的内存要求略高<br>
&ensp;&ensp;默认 metalogger 复制元数据时间较长（可调整）<br>
- 内存使用问题<br>
&ensp;&ensp;对于 master 服务器来说，资源型要求就是内存大小，为了整个系统访问更快，mfs 会把所以访问的元数据 metadada 信息放在内存中提供用户访问，因此，当文件数量增加时，内存使用量就会增加， 根据官方说法，处理一百万个文件 chunkserver，大概需要 300M 的内存空间。据此，推算如果未来要出来 1 个亿的文件 chunkserver，大概需要 30G 内存空间<br>
- MFS 的应用场景<br>
&ensp;&ensp;大规模高并发的线上数据存储及访问（小文件，大文件都适合）<br>
&ensp;&ensp;大规模的数据处理，如日志分析，小文件强调性能不用 HDFS<br>
## 3.2 MFS 组件说明
- 管理服务器<br>
&ensp;&ensp;managing server 简称 master ：这个组件的角色是管理整个 mfs 文件系统的主服务器，除了分发用户请求外，还用来存储整个文件系统中每个数据文件的 metadata 信息，metadate（元数据） 信息包括文件（也可以是目录，socket，管道，块设备等）的大小，属性，文件的位置路径等<br>
- 元数据备份服务器<br>
&ensp;&ensp;Metadata backup servers  简称 metalogger：这个组件的作用是备份管理服务器master 的变化的 metadata 信息日志文件，文件类型为 changelog_ml.*.mfs。以便于在管理服务器出问题时，可以经过简单的操作即可让新的主服务器进行工作<br>
- 数据存储服务器组<br>
&ensp;&ensp;data servers（chunk servers）简称data：这个组件就是真正存放数据文件实体的服务器了，这个角色可以有多台不同的物理服务器或不同的磁盘及分区来充当，当配置数据的副本多于一份时，据写入到一个数据服务器后，会根据算法在其他数据服务器上进行同步备份<br>
- 客户机服务器组<br>
&ensp;&ensp;（client servers）简称 client：这个组件就是挂载并使用 mfs 文件系统的客户端，当读写文件时，客户端首先会连接主管理服务器获取数据的 metadata 信息，然后根据得到的 metadata 信息， 访问数据服务器读取或写入文件实体，mfs 客户端通过 fuse mechanism 实现挂载 mfs 文件系统的，因此，只有系统支持fuse，就可以作为客户端访问 mfs 整个文件系统<br>
- MFS 拓扑图<br>
 <img width="704" height="547" alt="Linux：集群_47" src="https://github.com/user-attachments/assets/30406bd9-a68f-4e6e-894c-36817848574e" /><br><br>

- 数据操作组件之间的协同过程<br>

&ensp;&ensp;MFS 的读数据过程<br>
&ensp;&ensp;&ensp;&ensp;client 当需要一个数据时，首先向 master server 发起查询请求<br>
&ensp;&ensp;&ensp;&ensp;管理服务器检索自己的数据，把获取到数据所在的可用数据服务器位置 （ip|port|chunkid） 发送给客户端<br>
&ensp;&ensp;&ensp;&ensp;客户端向具体的数据服务器发起数据获取请求<br>
&ensp;&ensp;&ensp;&ensp;数据服务器将数据发送给客户端<br>
 <img width="720" height="537" alt="Linux：集群_48" src="https://github.com/user-attachments/assets/5ce07a05-2f66-4d25-a286-b39d2ceaac78" /><br>

&ensp;&ensp;MFS 的写数据过程<br>
&ensp;&ensp;&ensp;&ensp;客户端有数据写需求时，首先向管理服务器提供文件元数据信息请求存储地址（元数据信息如： 文件名|大小|份数等）<br>
&ensp;&ensp;&ensp;&ensp;管理服务器根据写文件的元数据信息，到数据服务器创建新的数据块<br>
&ensp;&ensp;&ensp;&ensp;数据服务器返回创建成功的消息<br>
&ensp;&ensp;&ensp;&ensp;管理服务器将数据服务器的地址返回给客户端(chunkIP|port|chunkid)<br>
&ensp;&ensp;&ensp;&ensp;客户端向数据服务器写数据<br>
&ensp;&ensp;&ensp;&ensp;数据服务器返回给客户端写成功的消息<br>
&ensp;&ensp;&ensp;&ensp;客户端将此次写完成结束信号和一些信息发送到管理服务器来更新文件的长度和最后修改时间<br>
 <img width="849" height="538" alt="Linux：集群_52" src="https://github.com/user-attachments/assets/ae26ec6f-91c0-4d4c-b8b0-62f0c4fbaf22" /><br>

&ensp;&ensp;MFS 的删除文件过程<br>
&ensp;&ensp;&ensp;&ensp;客户端有删除操作时，首先向 Master 发送删除信息<br>
&ensp;&ensp;&ensp;&ensp;Master 定位到相应元数据信息把删除的指令发给数据存储服务器 1<br>
&ensp;&ensp;&ensp;&ensp;数据存储服务器 1 将异步清理（等待设置中的设置的秒数之后再删除），异步清理时间到了之后数据存储服务器 1会把块数据删除，然后把删除信息发给管理服务器<br>
&ensp;&ensp;&ensp;&ensp;响应客户端删除成功的信号<br>
 <img width="836" height="551" alt="Linux：集群_50" src="https://github.com/user-attachments/assets/78b41b23-3287-41d3-8d2a-3c842086d0ef" /><br>

&ensp;&ensp;MFS 修改文件内容的过程<br>
&ensp;&ensp;&ensp;&ensp;客户端有修改文件内容时，首先向Master 发送操作信息<br>
&ensp;&ensp;&ensp;&ensp;管理服务器检索自己的数据，把获取到数据所在的可用数据服务器位置 （ip|port|chunkid） 发送给客户端<br>
&ensp;&ensp;&ensp;&ensp;客户端连接 chunk server， Master 申请新的块给.swp 文件<br>
&ensp;&ensp;&ensp;&ensp;客户端关闭文件后，会向 Master 发送关闭信息<br>
&ensp;&ensp;Master 会检测内容是否有更新，若有，则申请新的块存放更改后的文件，删除原有块和.swp 文件块，若无，则直接删除.swp 文件块<br>
 <&ensp;&ensp;img width="866" height="554" alt="Linux：集群_51" src="https://github.com/user-attachments/assets/85ccc28e-573a-4ec3-bfb2-cad3380810e8" /><br>




















