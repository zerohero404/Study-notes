# 1.集群化存储概述
## 1.1 用于存储的网络技术
- DAS Direct-Attached Storage<br>
&ensp;&ensp;说明：直接附加存储，等同于存储设备通过数据线、光缆、SATA 直连主机<br>
&ensp;&ensp;优点:技术简单、传输效率最高<br>
&ensp;&ensp;缺点:存储设备与主机相互绑定，不利于后期扩展与共享<br>
-NAS (Network Attached Storage)<br>
&ensp;&ensp;说明：网络附加存储，通过网络在存储主机与使用主机之间传输数据<br>
&ensp;&ensp;优点:技术相对简单、不要求存储设备直连本机，只需在局域网下即可<br>
&ensp;&ensp;缺点:存储速率较慢<br>
&ensp;&ensp;网络拓扑<br>
 <img width="428" height="390" alt="Linux：集群_31" src="https://github.com/user-attachments/assets/2f39aff1-d4f8-48e0-8f5f-51b0ea362a78" /><br>
- SAN (Storage Area Network)<br>
&ensp;&ensp;说明：存储区域网络，将生产网络与存储网络进行隔离，有效增加各部效率，减轻网络设备压力，适合大并发业务<br>
&ensp;&ensp;优点:存储安全性较高、存储速率较高<br>
&ensp;&ensp;缺点:造价昂贵、技术难度相对较高<br>
&ensp;&ensp;网络拓扑<br>
 <img width="638" height="478" alt="Linux：集群_32" src="https://github.com/user-attachments/assets/affab867-672d-4a19-8903-a3bb508389f4" /><br>
## 1.2 实现存储的技术分类
- 块存储<br>
&ensp;&ensp;常见设备:<br>
 <img width="326" height="341" alt="Linux：集群_33" src="https://github.com/user-attachments/assets/51b91231-7e45-4875-a190-588113c02226" /><br>
&ensp;&ensp;优点:可用通过 Raid、LVM 等简单技术实现高可用、可独立使用<br>
&ensp;&ensp;缺点:不利于在多台网络设备之间进行共享<br>
- 文件存储、网络存储<br>
&ensp;&ensp;常见方法:SAMBA、NFS<br>
&ensp;&ensp;优点:构建资金低、可在不同主机之间共享存储<br>
&ensp;&ensp;缺点:读写效率低，传输效率慢<br>
- 对象存储、分布式存储、存储桶<br>
&ensp;&ensp;常见设备:
 <img width="494" height="589" alt="Linux：集群_34" src="https://github.com/user-attachments/assets/4e3baf51-2357-43d8-8cd2-da8317a4dbdb" /><br>
&ensp;&ensp;优点:读写效率高、可在不同主机之间共享存储<br>
&ensp;&ensp;缺点:造价昂贵，技术实现难度较高<br>
- 常见的一些存储名词以及背后使用技术对应关系<br>
&ensp;&ensp;文件存储、网络存储  FTPSAMBA、Apache、Nginx<br>
&ensp;&ensp;对象存储、分布式存储、云存储  CEPH、MooseFS、RHCS、ClusterFS<br>
&ensp;&ensp;块存储  ISCSI<br>
## 1.3 存储实现技术与存储网络之间的联系
- 块存储 + DAS<br>
&ensp;&ensp;结构描述：块存储直连主机设备是最常见的存储方式，也是后边所有存储方式的基石<br>
&ensp;&ensp;结构拓扑:<br>
 <img width="672" height="464" alt="Linux：集群_35" src="https://github.com/user-attachments/assets/88e6c22a-9a5e-46b0-912d-158e8ca76e1e" /><br>
- 块存储 + NAS<br>
&ensp;&ensp;结构描述：将块设备通过局域网络共享至其它使用主机节点，提升空间利用率，便于后期扩展<br>
&ensp;&ensp;结构拓扑:<br>
 <img width="531" height="271" alt="Linux：集群_36" src="https://github.com/user-attachments/assets/1acc23e0-8959-49aa-9be6-89cc1818c22d" /><br>
- 块存储 + SAN<br>
&ensp;&ensp;结构描述：将块设备通过局域网络共享至其它使用主机节点，提升空间利用率。并且将用户访问网络与存储网络相隔离，利用提升存储效率以后后期维护扩展<br>
&ensp;&ensp;结构拓扑<br>
 <img width="713" height="453" alt="Linux：集群_37" src="https://github.com/user-attachments/assets/a564eb18-1f75-49f3-9e1b-b695fb2dc711" /><br>
- 文件存储 + NAS<br>
&ensp;&ensp;结构描述：利用共享服务将文件通过网络传输至使用设备，便于文件在多台机器之间的共享操作<br>
&ensp;&ensp;结构拓扑<br>
 <img width="680" height="398" alt="Linux：集群_38" src="https://github.com/user-attachments/assets/ef4a7eb3-3345-4e85-8fcb-1f701a991c1b" /><br>
- 对象存储 + SAN<br>
&ensp;&ensp;结构描述：利用分布式文件存储结构提升文件读写并发效率，并且将用户访问网络与存储网络相隔离， 利用提升存储效率以后后期维护扩展<br>
&ensp;&ensp;结构拓扑<br>
 <img width="767" height="399" alt="Linux：集群_39" src="https://github.com/user-attachments/assets/490e88c4-e596-4f68-b424-d81ed66517fa" /><br>

# 2. 文件存储 – NFS
## 2.1 NFS 概述
- 什么是 NFS ？<br>
&ensp;&ensp;NFS（Network File System）即网络文件系统，是 FreeBSD 支持的文件系统中的一种，它允许网络中的计算机之间通过TCP/IP 网络共享资源。在 NFS 的应用中，本地 NFS 的客户端应用可以透明地读写位于远端NFS 服务器上的文件，就像访问本地文件一样。NFS 最早是由Sun 公司发展出来的，后被逐渐完整以及整合至 Linux 内核<br>
- NFS 功能<br>
&ensp;&ensp;它就是是可以透过网络，让不同的主机、不同的操作系统可以共享存储<br>
- NFS 原理<br>
&ensp;&ensp;NFS 在文件传送或信息传送过程中依赖于 RPC 协议。远程过程调用 (Remote Procedure Call) 是能使客户端执行其他系统中程序的一种机制所以只要用到 NFS 的地方都要启动 RPC 服务，不论是 NFS SERVER 或者 NFS CLIENT 。这样 SERVER 和 CLIENT 才能通过 RPC 来实现 PROGRAM PORT的 对应。可以这么理解 RPC 和 NFS 的关系：NFS 是一个文件系统，而 RPC  是负责负责信息的传输<br>
- NFS 优点<br>
&ensp;&ensp;节省本地存储空间，将常用的数据存放在一台 NFS  服务器上且可以通过网络访问，那么本地终端将可以减少自身存储空间的使用<br>
&ensp;&ensp;用户不需要在网络中的每个机器上都建有 Home 目录，Home 目录可以放在 NFS 服务器上且可以在网络上被访问使用<br>
&ensp;&ensp;一些存储设备 CDROM 和 Zip 等都可以在网络上被别的机器使用。这可以减少整个网络上可移动介质设备的数量<br>
## 2.2 NFS 配置
- NFC 服务器
&ensp;&ensp;yum –y install rpcbind nfs-utils # 服务器端安装 rpcbind 和 NFS-utils 软件包
&ensp;&ensp;/etc/init.d/rpcbind status
&ensp;&ensp;先创建 /file/nfs-1 /file/nfs-2 两个文件夹
&ensp;&ensp;chown -R nobody:nobody /file/nfs-1/ /file/nfs-2/
&ensp;&ensp;vim /etc/exports # 里面文件是空白的

```bash
NFS 共享目录 NFS 客户端地址 1(参数 1,参数 2,参数 3……)  客户端地址 2(参数 1,参数 2,参数3……. )
NFS 共享目录 NFS 客户端地址(参数 1,参数 2,参数 3…… )
```
 <img width="380" height="42" alt="Linux：集群_40" src="https://github.com/user-attachments/assets/776e5caa-6e0e-47fd-9ef6-6feaec4f6d18" /><br>
&ensp;&ensp;service rpcbind restart<br>
&ensp;&ensp;service nfs restart<br>
&ensp;&ensp;showmount -e 10.10.10.11（NFC 服务器 IP） # 查看挂载情况<br>

- 客户端
&ensp;&ensp;mkdir /file-salve/nfs-1<br>
&ensp;&ensp;mkdir /file-salve/nfs-2<br>
&ensp;&ensp;先创建两个挂载文件夹<br>
&ensp;&ensp;chown -R nobody:nobody /file-salve/nfs-1/ /file-salve/nfs-2<br>
&ensp;&ensp;mount -t nfs 10.10.10.11（NFC 服务器 IP）:/file/nfs-1（文件路径） /file-salve/nfs-1/（本机要挂载的路径）<br>
&ensp;&ensp;mount -t nfs 10.10.10.11（NFC 服务器 IP）:/file/nfs-2（文件路径） /file-salve/nfs-2/（本机要挂载的路径）<br>
- /etc/exports 参数规则<br>
&ensp;&ensp;rw：read-write，可读写，注意，仅仅这里设置成读写客户端还是不能正常写入，还要正确地设置共享目录的权限<br>
&ensp;&ensp;ro：read-only，只读<br>
&ensp;&ensp;sync：文件同时写入硬盘和内存<br>
&ensp;&ensp;async：文件暂存于内存，而不是直接写入内存<br>
&ensp;&ensp;no_root_squash：NFS 客户端连接服务端时如果使用的是 root 的话，那么对服务端分享的目录来说， 也拥有 root 权限。显然开启这项是不安全的<br>
&ensp;&ensp;root_squash：NFS 客户端连接服务端时如果使用的是 root 的话，那么对服务端分享的目录来说，拥有匿名用户权限，通常他将使用 nobody 或 nfsnobody 身份<br>
&ensp;&ensp;all_squash：不论 NFS 客户端连接服务端时使用什么用户，对服务端分享的目录来说都是拥有匿名用户权限<br>
&ensp;&ensp;anonuid：匿名用户的 UID  值，通常是 nobody 或 nfsnobody，可以在此处自行设定<br>
&ensp;&ensp;anongid：匿名用户的GID 值<br>

# 3. 分布式存储 – MFS
## 3.1 什么是 MFS？
- MFS 相关介绍<br>
&ensp;&ensp;MooseFS 是一个具备冗余容错功能的分布式网络文件系统，它将数据分别存放在多个物理服务器或单独磁盘或分区上，确保一份数据有多个备份副本，然而对于访问 MFS 的客户端或者用户来说，整个分布式网络文件系统集群看起来就像一个资源一样，从其对文件系统的情况看 MooseFS 就相当于UNIX 的文件系统<br>
- MFS 的特性说明<br>
&ensp;&ensp;高可靠性：每一份数据可以设置多个备份（多分数据），并可以存储在不同的主机上<br>
&ensp;&ensp;高可扩展性：可以很轻松的通过增加主机的磁盘容量或增加主机数量来动态扩展整个文件系统的存储量<br>
&ensp;&ensp;高可容错性：我们可以通过对 mfs 进行系统设置，实现当数据文件被删除后的一段时间内，依旧存放于主机的回收站中，以备误删除恢复数据<br>
&ensp;&ensp;高数据一致性：即使文件被写入、访问时，我们依然可以轻松完成对文件的一致性快照<br>
- MFS 缺点<br>
&ensp;&ensp;master 目前是单点，虽然会把数据信息同步到备份服务器，但是恢复需要时间<br>
&ensp;&ensp;master 服务器对主机的内存要求略高<br>
&ensp;&ensp;默认 metalogger 复制元数据时间较长（可调整）<br>
- 内存使用问题<br>
&ensp;&ensp;对于 master 服务器来说，资源型要求就是内存大小，为了整个系统访问更快，mfs 会把所以访问的元数据 metadada 信息放在内存中提供用户访问，因此，当文件数量增加时，内存使用量就会增加， 根据官方说法，处理一百万个文件 chunkserver，大概需要 300M 的内存空间。据此，推算如果未来要出来 1 个亿的文件 chunkserver，大概需要 30G 内存空间<br>
- MFS 的应用场景<br>
&ensp;&ensp;大规模高并发的线上数据存储及访问（小文件，大文件都适合）<br>
&ensp;&ensp;大规模的数据处理，如日志分析，小文件强调性能不用 HDFS<br>
## 3.2 MFS 组件说明
- 管理服务器<br>
&ensp;&ensp;managing server 简称 master ：这个组件的角色是管理整个 mfs 文件系统的主服务器，除了分发用户请求外，还用来存储整个文件系统中每个数据文件的 metadata 信息，metadate（元数据） 信息包括文件（也可以是目录，socket，管道，块设备等）的大小，属性，文件的位置路径等<br>
- 元数据备份服务器<br>
&ensp;&ensp;Metadata backup servers  简称 metalogger：这个组件的作用是备份管理服务器master 的变化的 metadata 信息日志文件，文件类型为 changelog_ml.*.mfs。以便于在管理服务器出问题时，可以经过简单的操作即可让新的主服务器进行工作<br>
- 数据存储服务器组<br>
&ensp;&ensp;data servers（chunk servers）简称data：这个组件就是真正存放数据文件实体的服务器了，这个角色可以有多台不同的物理服务器或不同的磁盘及分区来充当，当配置数据的副本多于一份时，据写入到一个数据服务器后，会根据算法在其他数据服务器上进行同步备份<br>
- 客户机服务器组<br>
&ensp;&ensp;（client servers）简称 client：这个组件就是挂载并使用 mfs 文件系统的客户端，当读写文件时，客户端首先会连接主管理服务器获取数据的 metadata 信息，然后根据得到的 metadata 信息， 访问数据服务器读取或写入文件实体，mfs 客户端通过 fuse mechanism 实现挂载 mfs 文件系统的，因此，只有系统支持fuse，就可以作为客户端访问 mfs 整个文件系统<br>
- MFS 拓扑图<br>
 <img width="704" height="547" alt="Linux：集群_47" src="https://github.com/user-attachments/assets/30406bd9-a68f-4e6e-894c-36817848574e" /><br><br>

- 数据操作组件之间的协同过程<br>

&ensp;&ensp;MFS 的读数据过程<br>
&ensp;&ensp;&ensp;&ensp;client 当需要一个数据时，首先向 master server 发起查询请求<br>
&ensp;&ensp;&ensp;&ensp;管理服务器检索自己的数据，把获取到数据所在的可用数据服务器位置 （ip|port|chunkid） 发送给客户端<br>
&ensp;&ensp;&ensp;&ensp;客户端向具体的数据服务器发起数据获取请求<br>
&ensp;&ensp;&ensp;&ensp;数据服务器将数据发送给客户端<br>
 <img width="720" height="537" alt="Linux：集群_48" src="https://github.com/user-attachments/assets/5ce07a05-2f66-4d25-a286-b39d2ceaac78" /><br>

&ensp;&ensp;MFS 的写数据过程<br>
&ensp;&ensp;&ensp;&ensp;客户端有数据写需求时，首先向管理服务器提供文件元数据信息请求存储地址（元数据信息如： 文件名|大小|份数等）<br>
&ensp;&ensp;&ensp;&ensp;管理服务器根据写文件的元数据信息，到数据服务器创建新的数据块<br>
&ensp;&ensp;&ensp;&ensp;数据服务器返回创建成功的消息<br>
&ensp;&ensp;&ensp;&ensp;管理服务器将数据服务器的地址返回给客户端(chunkIP|port|chunkid)<br>
&ensp;&ensp;&ensp;&ensp;客户端向数据服务器写数据<br>
&ensp;&ensp;&ensp;&ensp;数据服务器返回给客户端写成功的消息<br>
&ensp;&ensp;&ensp;&ensp;客户端将此次写完成结束信号和一些信息发送到管理服务器来更新文件的长度和最后修改时间<br>
 <img width="849" height="538" alt="Linux：集群_52" src="https://github.com/user-attachments/assets/ae26ec6f-91c0-4d4c-b8b0-62f0c4fbaf22" /><br>

&ensp;&ensp;MFS 的删除文件过程<br>
&ensp;&ensp;&ensp;&ensp;客户端有删除操作时，首先向 Master 发送删除信息<br>
&ensp;&ensp;&ensp;&ensp;Master 定位到相应元数据信息把删除的指令发给数据存储服务器 1<br>
&ensp;&ensp;&ensp;&ensp;数据存储服务器 1 将异步清理（等待设置中的设置的秒数之后再删除），异步清理时间到了之后数据存储服务器 1会把块数据删除，然后把删除信息发给管理服务器<br>
&ensp;&ensp;&ensp;&ensp;响应客户端删除成功的信号<br>
 <img width="836" height="551" alt="Linux：集群_50" src="https://github.com/user-attachments/assets/78b41b23-3287-41d3-8d2a-3c842086d0ef" /><br>

&ensp;&ensp;MFS 修改文件内容的过程<br>
&ensp;&ensp;&ensp;&ensp;客户端有修改文件内容时，首先向Master 发送操作信息<br>
&ensp;&ensp;&ensp;&ensp;管理服务器检索自己的数据，把获取到数据所在的可用数据服务器位置 （ip|port|chunkid） 发送给客户端<br>
&ensp;&ensp;&ensp;&ensp;客户端连接 chunk server， Master 申请新的块给.swp 文件<br>
&ensp;&ensp;&ensp;&ensp;客户端关闭文件后，会向 Master 发送关闭信息<br>
&ensp;&ensp;Master 会检测内容是否有更新，若有，则申请新的块存放更改后的文件，删除原有块和.swp 文件块，若无，则直接删除.swp 文件块<br>
 <img width="866" height="554" alt="Linux：集群_51" src="https://github.com/user-attachments/assets/85ccc28e-573a-4ec3-bfb2-cad3380810e8" /><br>

&ensp;&ensp;MFS 重命名文件的过程<br>
&ensp;&ensp;&ensp;&ensp;客户端重命名文件时，会向 Master 发送操作信息<br>
&ensp;&ensp;&ensp;&ensp;Master 直接修改元数据信息中的文件名；返回重命名完成信息<br>
 <img width="832" height="545" alt="Linux：集群_49" src="https://github.com/user-attachments/assets/245db46a-a692-479f-944b-44df184bfc4d" /><br>

&ensp;&ensp;MFS 遍历文件的过程<br>
&ensp;&ensp;&ensp;&ensp;遍历文件不需要访问 chunk server，当有客户端遍历请求时，向 Master 发送操作信息<br>
&ensp;&ensp;&ensp;&ensp;Master 返回相应元数据信息，客户端接收到信息后显示<br>
 <img width="832" height="545" alt="Linux：集群_49" src="https://github.com/user-attachments/assets/245db46a-a692-479f-944b-44df184bfc4d" /><br>

&ensp;&ensp;补充描述<br>
&ensp;&ensp;&ensp;&ensp;Master 记录着管理信息，比如：文件路径|大小|存储的位置(ip,port,chunkid)|份数|时间等，元数据信息存在于内存中，会定期写入 metadata.mfs.back 文件中，定期同步到metalogger，操作实时写入changelog.*.mfs，实时同步到 metalogger 中。master 启动将 metadata.mfs 载入内存，重命名为metadata.mfs.back 文件<br>
&ensp;&ensp;&ensp;&ensp;文件以 chunk 大小存储，每 chunk 最大为 64M，小于 64M 的，该 chunk 的大小即为该文件大小（ 验证实际 chunk 文件略大于实际文件），超过 64M 的文件将被切分，以每一份（chunk）的大小不超过 64M 为原则；块的生成遵循规则：目录循环写入(00-FF 256 个目录循环，step 为 2)、chunk 文件递增生成、大文件切分目录连续<br>
&ensp;&ensp;&ensp;&ensp;Chunkserver 上的剩余存储空间要大于 1GB（Reference Guide 有提到），新的数据才会被允许写入，否则，你会看到 No space left on device 的提示，实际中，测试发现当磁盘使用率达到 95%左右的时候，就已经不行写入了，当时可用空间为 1.9GB<br>
&ensp;&ensp;&ensp;&ensp;文件可以有多份 copy，当 goal 为 1 时，文件会被随机存到一台 chunkserver 上，当 goal 的数大于 1时，copy 会由 master 调度保存到不同的chunkserver 上，goal 的大小不要超过 chunkserver 的数量，否则多出的 copy，不会有 chunkserver 去存<br>

## 3.3 MFS 服务集群搭建

- mfsmaster.cfg 主配置配置文件说明解释<br>
&ensp;&ensp;WORKING_USER = mfs # 工作用户<br>
&ensp;&ensp;WORKING_GROUP = mfs # 工作用户组<br>
&ensp;&ensp;SYSLOG_IDENT = mfsmaster #在 syslog 中的表示， 说明这是 mfsmaster 产生的<br>
&ensp;&ensp;LOCK_MEMORY = 0 # 是否执行 mlockall()以避免 mfsmaster 内存溢出（默认为 0）<br>
&ensp;&ensp;NICE_LEVEL = -19 #  运行的优先级（默认-19， 注意， 这进程必须是 root 启动 ）<br>
&ensp;&ensp;DATA_PATH = /usr/local/mfs/var/mfs # 数 据 存 放 路 径 ， 该 目 录 下 有 三 大 类 文件 ：changelog，sessions，stats<br>
&ensp;&ensp;EXPORTS_FILENAME = /usr/local/mfs/etc/mfs/mfsexports.cfg #被挂载目录以及其权限控制文件的存放位置<br>
&ensp;&ensp;BACK_LOGS = 50 #元数据的改变日志文件数量（默认是 50）<br>
&ensp;&ensp;MATOML_LISTEN_HOST = * # 元数据日志服务器监听的 IP 地址（默认是*， 代表任何 IP）<br>
&ensp;&ensp;MATOML_LISTEN_PORT = 9419 # 元数据日志服务器监听的端口地址， 默认是 9419<br>
&ensp;&ensp;MATOCS_LISTEN_HOST = * # 用于存储服务器（Chunk Server） 连接的 IP 地址<br>
&ensp;&ensp;MATOCS_LISTEN_PORT = 9420 # 是存储服务器（Chunk server） 连接的端口地址<br>
&ensp;&ensp;REPLICATIONS_DELAY_INIT = 300 # 延 迟 复 制 的 时 间 （ 默 认 是 300）<br>
&ensp;&ensp;CHUNKS_LOOP_MIN_TIME = 300 # chunks 的回环率<br>
&ensp;&ensp;CHUNKS_SOFT_DEL_LIMIT = 10<br>
&ensp;&ensp;CHUNKS_WRITE_REP_LIMIT = 2,1,1,4 在一个循环里复制到一个 CHUNK Server  的最大 chunks 数目<br>
&ensp;&ensp;CHUNKS_READ_REP_LIMIT = 10,5,2,5<br>
&ensp;&ensp;MATOCL_LISTEN_PORT = 9421 # 客户端连接端口<br>
- 拓扑图:<br>
 <img width="1052" height="290" alt="Linux：集群_41" src="https://github.com/user-attachments/assets/a9dcbcba-abd7-4320-923d-443e6254988f" /><br>
&ensp;&ensp;moosefs-3.0.51-1.tar 下载地址<br>
&ensp;&ensp;&ensp;&ensp;wget http://ppa.moosefs.com/src/moosefs-3.0.51-1.tar.gz<br>

- MFS 管理服务器搭建<br>
&ensp;&ensp;service iptables stop<br>
&ensp;&ensp;chkconfig iptables off<br>
&ensp;&ensp;yum -y install gcc*<br>
&ensp;&ensp;useradd -s /sbin/nologin -M mfs # 创建 MFS 用户，用于 MFS 运行身份<br>
&ensp;&ensp;wget http://ppa.moosefs.com/src/moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;tar -xf moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;cd moosefs-3.0.51<br>

```bash
./configure –prefix=/usr/local/mfs –with-default-user=mfs –with-default-group=mfs
make && make install
```

&ensp;&ensp;chmod a+x /usr/local/mfs/sbin/* # 优化配置，使 MFS 命令便于调用<br>
&ensp;&ensp;ln -s /usr/local/mfs/sbin/* /usr/local/sbin/<br>
&ensp;&ensp;ln -s /usr/local/mfs/bin/* /usr/local/bin/<br>
&ensp;&ensp;cd /usr/local/mfs/etc/mfs<br>
&ensp;&ensp;cp -a mfsmaster.cfg.sample mfsmaster.cfg # mfsmaster.cfg 就是主配置文件<br>
&ensp;&ensp;cp -a mfsexports.cfg.sample mfsexports.cfg # 设置 MFS 挂载权限<br>
&ensp;&ensp;vim mfsexports.cfg<br>

```bash
在最下面添加
10.10.10.0/24（允许访问的网段） . （MFS文件系统）rw（读写权限）,alldirs（允许任何目录挂载）,maproot=0（登录之后是 root 权限）
```
&ensp;&ensp;cp -a /usr/local/mfs/var/mfs/metadata.mfs.empty /usr/local/mfs/var/mfs/metadata.mfs # 拷贝MFS 元数据信息文件（初始）<br>
&ensp;&ensp;mfsmaster start # 启动 MFS-Master 服务<br>
&ensp;&ensp;mfscgiserv # 开启网页查看 MFS 管理服务器<br>
&ensp;&ensp;在浏览器输入10.10.10.11（MFS 管理服务器）:9425<br>

- MFS 元数据备份服务器搭建<br>
&ensp;&ensp;service iptables stop<br>
&ensp;&ensp;chkconfig iptables off<br>
&ensp;&ensp;yum -y install gcc*<br>
&ensp;&ensp;useradd -s /sbin/nologin -M mfs # 创建 MFS 用户，用于 MFS 运行身份<br>
&ensp;&ensp;wget http://ppa.moosefs.com/src/moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;tar -xf moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;cd moosefs-3.0.51<br>

```bash
./configure –prefix=/usr/local/mfs –with-default-user=mfs –with-default-group=mfs
make & make install
```

&ensp;&ensp;chmod a+x /usr/local/mfs/sbin/* # 优化配置，使 MFS 命令便于调用<br>
&ensp;&ensp;ln -s /usr/local/mfs/sbin/* /usr/local/sbin/<br>
&ensp;&ensp;ln -s /usr/local/mfs/bin/* /usr/local/bin/<br>
&ensp;&ensp;cd /usr/local/mfs/etc/mfs/<br>
&ensp;&ensp;cp -a mfsmetalogger.cfg.sample mfsmetalogger.cfg<br>
&ensp;&ensp;vim mfsmetalogger.cfg<br>

```bash
修改这两个选项
META_DOWNLOAD_FREQ = 2 # 设置完整同步间隔为 2  小时
MASTER_HOST = 10.10.10.11 # 设置 MFS-Master 服务器 IP 地址
```

&ensp;&ensp;mfsmetalogger start # 启动 MFS-Logger 服务<br>
&ensp;&ensp;lsof -i :9419 # 查看 9419 端口连接情况<br>
&ensp;&ensp;cd /usr/local/mfs/var/mfs/ && ls # 查看 MFS 元数据备份文件<br>

- MFS 数据存储服务器搭建<br>
&ensp;&ensp;数据存储服务器 1 和 2都这样设置<br>
&ensp;&ensp;service iptables stop<br>
&ensp;&ensp;chkconfig iptables off<br>
&ensp;&ensp;yum -y install gcc*<br>
&ensp;&ensp;useradd -s /sbin/nologin -M mfs # 创建 MFS 用户，用于 MFS 运行身份<br>
&ensp;&ensp;wget http://ppa.moosefs.com/src/moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;tar -xf moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;cd moosefs-3.0.51<br>

```bash
./configure –prefix=/usr/local/mfs –with-default-user=mfs –with-defaultgroup=mfs && make && make install
```

&ensp;&ensp;chmod a+x /usr/local/mfs/sbin/* # 优化配置，使 MFS 命令便于调用<br>
&ensp;&ensp;ln -s /usr/local/mfs/sbin/* /usr/local/sbin/<br>
&ensp;&ensp;ln -s /usr/local/mfs/bin/* /usr/local/bin/<br>
&ensp;&ensp;cd /usr/local/mfs/etc/mfs/<br>
&ensp;&ensp;cp -a mfschunkserver.cfg.sample mfschunkserver.cfg # 拷贝 Chunkserver 配置文件<br>
&ensp;&ensp;vim mfschunkserver.cfg<br>

```bash
修改
MASTER_HOST = 10.10.10.11 # 指定 MFS-Master 服务器地址
MASTER_PORT = 9420 # 指定 MFS-Master 服务器端口
HDD_CONF_FILENAME = /usr/local/mfs/etc/mfs/mfshdd.cfg # 指定提供存储的配置文件位置
```

&ensp;&ensp;vim /usr/local/mfs/etc/mfs/mfshdd.cfg # 修改 MFS  服务 HDD 路径<br>

```bash
添加
/mfschunk # 挂载点目录
```

&ensp;&ensp;mkdir /mfschunk # 创建挂载点目录<br>
&ensp;&ensp;新加硬盘做 MFS 服务器存储目录<br>
&ensp;&ensp;fdisk -l # 查看新加硬盘信息<br>
&ensp;&ensp;fdisk /dev/sdb<br>
&ensp;&ensp;&ensp;&ensp;输入 n 新建分区<br>
&ensp;&ensp;&ensp;&ensp;输入 p 主分区<br>
&ensp;&ensp;&ensp;&ensp;输入 1 设置为 1 号分区<br>
&ensp;&ensp;&ensp;&ensp;按两下回车将全部硬盘都设置为 1 号分区<br>
&ensp;&ensp;&ensp;&ensp;输入 W 保存<br>
&ensp;&ensp;mkfs.ext4 /dev/sdb1 # 以 ext4 文件格式格式化新加硬盘<br>
&ensp;&ensp;mount -t ext4 /dev/sdb1 /mfschunk/ # 将新加硬盘挂载到 MFS 存储服务器的挂载点目录<br>
&ensp;&ensp;chown -R mfs:mfs /mfschunk/<br>
&ensp;&ensp;mfschunkserver start #  启动 MFSChunkserver 服务<br>

- MFS 客户端搭建<br>
&ensp;&ensp;service iptables stop<br>
&ensp;&ensp;chkconfig iptables off<br>
&ensp;&ensp;yum -y install gcc*<br>
&ensp;&ensp;useradd -s /sbin/nologin -M mfs # 创建 MFS 用户，用于 MFS 运行身份<br>
&ensp;&ensp;wget http://ppa.moosefs.com/src/moosefs- 3.0.51-1.tar.gz<br>
&ensp;&ensp;tar -xf moosefs-3.0.51-1.tar.gz<br>
&ensp;&ensp;cd moosefs-3.0.51<br>

```bash
./configure –prefix=/usr/local/mfs –with-default-user=mfs –with-defaultgroup=mfs –disable-mfsmaster –disable-mfschunkserver
make && make install
```

&ensp;&ensp;chmod a+x /usr/local/mfs/sbin/* # 优化配置，使 MFS 命令便于调用<br>
&ensp;&ensp;ln -s /usr/local/mfs/sbin/* /usr/local/sbin/<br>
&ensp;&ensp;ln -s /usr/local/mfs/bin/* /usr/local/bin/<br>
&ensp;&ensp;mkdir /mnt/mfsclient # 建立 MFS 服务集群文件挂载点<br>
&ensp;&ensp;mfsmount /mnt/mfsclient/ -H 10.10.10.11（MFS 管理服务器 IP 地址）<br>
&ensp;&ensp;chown -R mfs:mfs /mnt/mfsclient/<br>
&ensp;&ensp;cd /mnt/mfsclient/ && touch 1.txt && echo “abcd” >> 1.txt # 模拟写入数据<br>
&ensp;&ensp;mfsfileinfo 1.txt # 查看数据的具体信息以及存放位置<br>

## 3.4 MFS 服务器后期维护操作
- 以下操作都是在 MFS 客户端使用<br>
- 误删除处理，垃圾回收站机制<br>
&ensp;&ensp;给文件设置误删除保护时间<br>
&ensp;&ensp;&ensp;&ensp;mfssettrashtime 64800（时间） /mnt/mfsclient（MFS 客户端挂载 MFS 服务器集群文件挂载点路径）/1.txt（文件名）<br>
&ensp;&ensp;挂载回收站文件<br>
&ensp;&ensp;&ensp;&ensp;mkdir  /back # 创建回收站文件存放目录<br>
&ensp;&ensp;&ensp;&ensp;chown mfs:mfs /mfsback<br>
&ensp;&ensp;&ensp;&ensp;mfsmount -m /mfsback -H 10.10.10.11（MFS 管理服务器 IP 地址） # 挂载 MFS 回收站<br>
&ensp;&ensp;恢复误删除文件<br>
&ensp;&ensp;&ensp;&ensp;cd /mfsback/trash<br>
&ensp;&ensp;&ensp;&ensp;tree . >> /root/2.txt # 将垃圾箱文件目录树输出到 /root/2.txt 文件里<br>
&ensp;&ensp;&ensp;&ensp;vim /root/2.txt 找到误删除的文件<br>
&ensp;&ensp;&ensp;&ensp;mv 002/00000002\|1.txt（误删除文件路径） /back/trash/undel/<br>
&ensp;&ensp;快照功能<br>
&ensp;&ensp;&ensp;&ensp;cd /mnt/mfsclient（MFS 客户端挂载 MFS 服务器集群文件挂载点路径）<br>
&ensp;&ensp;&ensp;&ensp;mfsmakesnapshot 1.txt（文件名） /etc（快照存放的位置）<br>
&ensp;&ensp;&ensp;&ensp;cd /etc && ll | grep 1.txt # 快照功能类似软连接<br>
&ensp;&ensp;冗余 goal 设置（就是文件在 MFS 服务器集群几个存储服务器保存一份）<br>
&ensp;&ensp;&ensp;&ensp;mfssetgoal 2（份数）/mnt/mfsclient（MFS 客户端挂载 MFS 服务器集群文件挂载点路径）/1.txt（文件名）<br>
&ensp;&ensp;&ensp;&ensp;mfsfileinfo     /mnt/mfsclient/1.txt # 查看数据的具体信息以及存放位置<br>
&ensp;&ensp;还原 master 服务器<br>
&ensp;&ensp;&ensp;&ensp;将元数据备份服务器下的 /usr/local（前面是你 mfs 安装路径）/mfs/var/mfs 的所有文件复制到 MFS 管理服务器 /usr/local（前面是你 mfs 安装路径）/mfs/var/mfs 下<br>
&ensp;&ensp;&ensp;&ensp;然后在 MFS 管理服务器输入 mfsmaster -a 还原 MFS 管理服务器<br>

# 4 块存储之 ISCSI   服务
## 4.1 ISCSI 概述
- ISCSI 与 SCSI 原理差别<br>
 <img width="282" height="269" alt="Linux：集群_42" src="https://github.com/user-attachments/assets/940f4617-3772-48c0-8eb3-092221fa149e" /><br>
- 常见用于 ISCSI 服务的网络拓扑类型<br>
&ensp;&ensp;SAN：Storage Area Network，存储区域网络，多采用高速光纤通道，对速率、冗余性要求高<br>
 <img width="465" height="217" alt="Linux：集群_43" src="https://github.com/user-attachments/assets/36fdc851-344c-4896-be1a-46519d6c8841" /><br>
&ensp;&ensp;Network Attachment Storage，网络附加存储，采用普通以太网，对速率、冗余无特别要求<br>
 <img width="213" height="161" alt="Linux：集群_44" src="https://github.com/user-attachments/assets/a2bda0af-69cd-4c4e-bf61-671c630561a5" /><br>
&ensp;&ensp;ISCSI 工作方式<br>
&ensp;&ensp;&ensp;&ensp;工作模式 C/S 结构<br>
&ensp;&ensp;&ensp;&ensp;服务（设备）端— target<br>
&ensp;&ensp;&ensp;&ensp;客户（应用）端— initiator<br>
&ensp;&ensp;工作拓扑图<br>
 <img width="611" height="273" alt="Linux：集群_45" src="https://github.com/user-attachments/assets/ebe78e0f-14f7-4355-a713-8e7236d96d9a" /><br>
 
## 4.2 ISCSI 服务构建
&ensp;&ensp;命令行方式设置 target（服务端）端 – 临时构建（理解过程）<br>
&ensp;&ensp;service iptables stop<br>
&ensp;&ensp;chkconfig iptables off<br>
&ensp;&ensp;给虚拟机新加了一块硬盘<br>
&ensp;&ensp;pvcreate /dev/sdb # 创建物理卷<br>
&ensp;&ensp;vgcreate rd0（物理卷组名） /dev/sdb # 创建物理卷组<br>
&ensp;&ensp;lvcreate -L 1G（逻辑卷大小） -n  ld0（逻辑卷名） rd0（物理卷组名） # 创建逻辑卷<br>
&ensp;&ensp;mkfs -t ext4 /dev/rd0/ld0  # 对创建的逻辑卷进行格式化<br>
&ensp;&ensp;yum -y install scsi-target-utils  # 安装包，创建 iSCSI 对象<br>
&ensp;&ensp;service tgtd start    # 开启服务<br>
&ensp;&ensp;netstat -anpt | grep tgtd     # 查看服务是否开启<br>
&ensp;&ensp;tgtadm -L （指定驱动类型 iscsi ) iscsi -o （制定操作类型，new为新建）new -m（制定管理的对象，对象是target） target -t(制定当前存储资源 id 号 1）1 -T 制定 iqn 标签，命名规则：iqn.创建年-月.域名反写：自定义）iqn.2020-10.com.zhangxu.www:zhuge<br>
&ensp;&ensp;不带解释的命令：tgtadm -L iscsi -o new -m target -t 1 -T iqn.2020-10.com.zhangxu.www:zhuge # 在 target 端创建 IQN 标签<br>
&ensp;&ensp;tgtadm -L iscsi -o show -m target # 查看 target 创建的信息<br>
&ensp;&ensp;tgtadm -L（指定驱动类型） iscsi -o new -m（管理的对象） logicalunit -t （tat 标签）1 -l（逻辑存储单元的标签） 1 -b（设备名称） /dev/rd0/ld0 # 绑定 IQN 标签到存储设备<br>
&ensp;&ensp;tgtadm -L iscsi -o bind -m target -t 1 -I 10.10.10.12 # 允许 IP 地址为 10.10.10.12 访问 存储资源 id 号为 1 绑定的存储设备<br>
&ensp;&ensp;设置 target（服务端）端 – 永久生效（生产环境使用）<br>
&ensp;&ensp;ervice iptables stop<br>
&ensp;&ensp;chkconfig iptables off<br>
&ensp;&ensp;给虚拟机新加了一块硬盘<br>
&ensp;&ensp;pvcreate /dev/sdb  # 创建物理卷<br>       
&ensp;&ensp;vgcreate rd0（物理卷组名） /dev/sdb   # 创建物理卷组<br>    
&ensp;&ensp;lvcreate -L 1G（逻辑卷大小） -n  ld0（逻辑卷名） rd0（物理卷组名） # 创建逻辑卷<br>
&ensp;&ensp;mkfs -t ext4 /dev/rd0/ld0 # 对创建的逻辑卷进行格式化<br>
&ensp;&ensp;yum -y install scsi-target-utils  # 安装包，创建 iSCSI 对象<br>
&ensp;&ensp;vim /etc/tgt/targets.conf<br>
&ensp;&ensp;添加：<br>

```bash
<target iqn.2020-10.com.zhangxu.www:zhuge（IQN 标签）>
    <backing-store /dev/rd0/ld0（IQN 标签要绑定的存储设备）>
    vendor_id zhangxu   （个人标签，可以随便写）
    lun 6 （存储资源 id 号）
    </backing-store>
    initiator-address 10.10.10.0/24 （允许访问的 IP 地址或者 IP 地址域）
</target>
```

&ensp;&ensp;initiator（客户端）端
&ensp;&ensp;yum -y install iscsi-initiator-utils
&ensp;&ensp;iscsiadm -m discovery -t st -p （target（服务端）端 IP 地址）10.10.10.11
&ensp;&ensp;iscsiadm -m node -T iqn.2020-10.com.zhangxu.www:zhuge –login #  在客户端进行挂载
&ensp;&ensp;vim /etc/fstab # 永久挂载
&ensp;&ensp;添加

```bash
/dev/sdb /mnt       ext4       defaults,_netdev   0 0
```

&ensp;&ensp;iscsiadm -m node -T iqn.2020-10.com.zhangxu.www:zhuge –logout     # 在客户端进行卸载



